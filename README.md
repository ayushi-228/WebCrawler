# Web Crawler

## Description
This Python project is a web crawler built using Scrapy that crawls Wikipedia pages related to Apple Inc. The crawler starts from the Wikipedia page for Apple Inc. and recursively follows links to other Wikipedia pages to gather information. The crawled data includes the title of each page and the text content of paragraphs within each page.

## Apple_crawler
1. Clone the repository:
   ```bash
   git clone https://github.com/ayushi-228/WebCrawler.git
   ```

3. Navigate to the project directory:
    ```bash
    cd Apple_crawler
    ```

4. (Optional) Create and activate a virtual environment:
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

5. Install Scrapy:
    ```bash
    pip install scrapy
    ```

## Usage
To run the web crawler, execute the following command:
```bash
scrapy crawl apple_inc -o output.json
```

## Spider
```bash
The `AppleSpider` class in this project is a Scrapy Spider specifically designed to crawl Wikipedia pages related to Apple Inc. It starts from the Wikipedia page for Apple Inc. (`https://en.wikipedia.org/wiki/Apple_Inc.`) and recursively follows links to other Wikipedia pages to gather information.

The Spider class has the following attributes and methods:

- `name`: The name of the Spider, which is `"apple_inc"`.
- `allowed_domains`: The list of allowed domains for the Spider to crawl, which includes `"en.wikipedia.org"`.
- `custom_settings`: A dictionary containing custom settings for the Spider, such as the maximum depth to crawl (`DEPTH_LIMIT`) and the maximum number of pages to crawl (`MAX_PAGES`).
- `__init__()`: The constructor method initializes the Spider object and sets the initial page count.
- `start_requests()`: A method that generates the initial requests to start crawling from the specified start URLs.
- `parse()`: The main parsing method that extracts data from each crawled page and yields the extracted data as Scrapy Items. It also follows links to other Wikipedia pages for further crawling.

The Spider class is responsible for crawling Wikipedia pages, extracting relevant information such as page titles and paragraph text, and following links to continue crawling recursively.
For more details on how the Spider works, refer to the code in `apple_spider.py`.
```
# Index Generator

## Description
The HTML Parser is a Python script designed to parse HTML files, extract relevant information, and save it as JSON data. This parser is specifically tailored for parsing HTML files generated by the Wikipedia Crawler project, but it can be adapted for other HTML structures as well.

## Installation

1. Navigate to the project directory:
    ```bash
    cd html_parser
    ```

2. (Optional) Create and activate a virtual environment:
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3. Install dependencies:
    ```bash
    pip install pandas beautifulsoup4
    ```

## Usage
 ```bash
To use the HTML Parser, follow these steps:

1. Place your HTML files in a directory.
2. Update the `html_directory` variable in the `main` block of `html_parser.py` to point to the directory containing your HTML files.
3. Specify the output JSON file path by updating the `output_filename` variable in the `HTMLParser` class.
4. Run the parser script:
    ```bash
    python html_parser.py
    ```

The parser will read the HTML files from the specified directory, extract titles and contents, and save the data as JSON in the specified output file.
 ```
## TFidf_generator
The TF-IDF Score Generator is a Python script that calculates TF-IDF (Term Frequency-Inverse Document Frequency) scores for paragraphs extracted from text data. TF-IDF is a numerical statistic used to reflect the importance of a term in a collection of documents.

1. Navigate to the project directory:
    ```bash
    cd tfidf_score_generator
    ```

2. (Optional) Create and activate a virtual environment:
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3. Install dependencies:
    ```bash
    pip install scikit-learn pandas
    ```

## Usage
To use the TF-IDF Score Generator, follow these steps:

1. Prepare your data:
   - Ensure your data is in JSON format with paragraphs stored in a field (e.g., `"paragraphs"`).
   - Update the `data_file` variable in the script to point to your JSON file.

2. Run the script:
    ```bash
    python tfidf_score_generator.py
    ```

3. Output files:
   - The script will generate two serialized files:
     - `tfidf.pkl`: Contains the serialized TF-IDF vectorizer and matrix.
     - `cosine_similarity.pkl`: Contains the serialized cosine similarities matrix.
# Loader

## Description
The Flask Search Application is a web-based search engine built using Flask, a micro web framework for Python. It provides a user-friendly interface for users to search for information based on their queries. The search functionality is powered by TF-IDF (Term Frequency-Inverse Document Frequency) scores generated by the TF-IDF Score Generator.


1. Navigate to the project directory:
    ```bash
    cd flask_search_app
    ```

2. (Optional) Create and activate a virtual environment:
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3. Install Flask and dependencies:
    ```bash
    pip install flask
    ```

## Usage
To use the Flask Search Application, follow these steps:

1. Ensure you have generated TF-IDF scores and saved them using the TF-IDF Score Generator.

2. Update the `tfidf_loader.py` script to load TF-IDF scores from the serialized files generated by the TF-IDF Score Generator.

3. Run the Flask application:
    ```bash
    python Flask.py
    ```

4. Open a web browser and navigate to `http://127.0.0.1:5000/` to access the search interface.

5. Enter a search query and press Enter or click the Search button.

6. View the search results displayed as clickable links.

## Files
```bash
- `app.py`: The main Flask application script containing routes and logic for processing search queries.
- `search.html`: HTML template file for the search interface.
- `tfidf_loader.py`: Python script to load TF-IDF scores generated by the TF-IDF Score Generator.
  ```
## TF-IDF Search Function

## Description
The TF-IDF Search Function is a Python script that implements a search functionality based on TF-IDF (Term Frequency-Inverse Document Frequency) scores and cosine similarity. It allows users to search for relevant documents based on a given query.

## Usage
1. Ensure you have the required data and files:
   - TF-IDF vectorizer and matrix serialized using `pickle`.
   - Cosine similarities matrix serialized using `pickle`.
   - Document data in a Pandas DataFrame or any suitable data structure.

2. Update the file paths in the script:
   - `tfidf_index_file`: Path to the serialized TF-IDF vectorizer and matrix.
   - `cosine_similarities_file`: Path to the serialized cosine similarities matrix.
   - `file_path`: Path to the document data file (e.g., CSV, JSON).

3. Load the indexed data using the `load_data()` function:
   ```python
   tfidf_vectorizer, tfidf_matrix, cosine_similarities = load_data(
       tfidf_index_file, cosine_similarities_file)


4. Load the document data into a suitable data structure (e.g., Pandas DataFrame).
5. Use the search() function to perform a search:
      ```python
      results = search(query, tfidf_vectorizer, tfidf_matrix, cosine_similarities, documents, top_k)
`query`: The search query provided by the user.
`tfidf`_vectorizer: The TF-IDF vectorizer object.
`tfidf_matrix`: The TF-IDF matrix.
`cosine_similarities`: The cosine similarities matrix.
`documents`: The document data (e.g., Pandas DataFrame).
`top_k`: The number of top results to return.


6. The `search_query()` function provides a convenient interface for performing searches:
   ```python
   results = search_query(query)


